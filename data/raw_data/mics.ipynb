{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:17:34.370125Z",
     "start_time": "2025-12-14T09:17:33.292307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# 1. LOAD DATA\n",
    "# Recent phishing (label 1)\n",
    "recent_phish = pd.read_csv(\"verified_online.csv\")\n",
    "# If it has more columns, assume 'url' exists:\n",
    "if \"url\" in recent_phish.columns:\n",
    "    recent_phish = recent_phish[\"url\"]\n",
    "recent_phish_df = pd.DataFrame({\"url\": recent_phish, \"label\": 1})\n",
    "\n",
    "# Old data with labels (0/1)\n",
    "old_data = pd.read_csv(\"more_data.csv\").drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "old_phish_df = old_data[old_data[\"label\"] == 1][[\"url\"]].copy()\n",
    "old_phish_df[\"label\"] = 1\n",
    "old_benign_df = old_data[old_data[\"label\"] == 0][[\"url\"]].copy()\n",
    "old_benign_df[\"label\"] = 0\n",
    "\n",
    "# Benign from Tranco top 100k\n",
    "tranco = pd.read_csv(\"top-1m.csv\", header=None, names=[\"rank\", \"domain\"])\n",
    "benign_urls_tranco = \"https://\" + tranco.iloc[:100000][\"domain\"].astype(str).str.strip()\n",
    "benign_tranco_df = pd.DataFrame({\"url\": benign_urls_tranco, \"label\": 0})\n",
    "\n",
    "# Combine benign sources (Tranco + old benign)\n",
    "benign_df = pd.concat([benign_tranco_df, old_benign_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# 2. DEDUP & CLEAN COLLISIONS\n",
    "# Deduplicate within each pool\n",
    "recent_phish_df = recent_phish_df.drop_duplicates(subset=\"url\")\n",
    "old_phish_df    = old_phish_df.drop_duplicates(subset=\"url\")\n",
    "benign_df       = benign_df.drop_duplicates(subset=\"url\")\n",
    "\n",
    "# Remove any benign URLs that accidentally appear in phishing sets\n",
    "phish_urls_all = pd.concat([recent_phish_df[[\"url\"]], old_phish_df[[\"url\"]]])[\"url\"].unique()\n",
    "benign_df = benign_df[~benign_df[\"url\"].isin(phish_urls_all)].copy()\n",
    "\n",
    "# Remove overlap between old + recent phishing (prioritize \"recent\" as phishing)\n",
    "recent_urls = set(recent_phish_df[\"url\"])\n",
    "old_phish_df = old_phish_df[~old_phish_df[\"url\"].isin(recent_urls)].copy()\n",
    "\n",
    "print(\"Recent phishing:\", len(recent_phish_df))\n",
    "print(\"Old phishing   :\", len(old_phish_df))\n",
    "print(\"Benign total   :\", len(benign_df))\n",
    "\n",
    "\n",
    "# 3. SPLIT RECENT PHISH INTO TRAIN / VAL / TEST\n",
    "recent_phish_df = recent_phish_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "n_recent = len(recent_phish_df)\n",
    "\n",
    "# You can tune these fractions if you want\n",
    "val_frac  = 0.10\n",
    "test_frac = 0.20\n",
    "n_val_phish  = int(n_recent * val_frac)\n",
    "n_test_phish = int(n_recent * test_frac)\n",
    "n_train_recent_phish = n_recent - n_val_phish - n_test_phish\n",
    "if n_train_recent_phish <= 0:\n",
    "    raise ValueError(\"Not enough recent phishing URLs for chosen splits.\")\n",
    "\n",
    "val_recent_df   = recent_phish_df.iloc[:n_val_phish]\n",
    "test_recent_df  = recent_phish_df.iloc[n_val_phish:n_val_phish + n_test_phish]\n",
    "train_recent_df = recent_phish_df.iloc[n_val_phish + n_test_phish:]\n",
    "\n",
    "print(\"\\nRecent phishing split:\")\n",
    "print(\"  Train recent phish:\", len(train_recent_df))\n",
    "print(\"  Val phish         :\", len(val_recent_df))\n",
    "print(\"  Test phish        :\", len(test_recent_df))\n",
    "\n",
    "\n",
    "# 4. BUILD PHISH TRAIN POOL\n",
    "# Training phishing = old + some recent (the remainder after val/test)\n",
    "phish_train_pool = pd.concat([old_phish_df, train_recent_df], ignore_index=True)\n",
    "phish_train_pool = phish_train_pool.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "print(\"\\nTotal phishing available for TRAIN:\", len(phish_train_pool))\n",
    "\n",
    "# 5. ALLOCATE BENIGN TO VAL & TEST FIRST (NO OVERLAP)\n",
    "\n",
    "\n",
    "benign_df = benign_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "benign_remaining = benign_df.copy()\n",
    "\n",
    "# VAL: 1 : 5 (phish : benign)\n",
    "n_val_benign_target = 5 * len(val_recent_df)\n",
    "n_val_benign = min(n_val_benign_target, len(benign_remaining))\n",
    "val_benign_df = benign_remaining.iloc[:n_val_benign].copy()\n",
    "benign_remaining = benign_remaining.iloc[n_val_benign:].copy()\n",
    "\n",
    "# TEST: 1 : 10 (phish : benign)\n",
    "n_test_benign_target = 10 * len(test_recent_df)\n",
    "n_test_benign = min(n_test_benign_target, len(benign_remaining))\n",
    "test_benign_df = benign_remaining.iloc[:n_test_benign].copy()\n",
    "benign_remaining = benign_remaining.iloc[n_test_benign:].copy()\n",
    "\n",
    "print(\"\\nVAL benign:\", len(val_benign_df), \" (target:\", n_val_benign_target, \")\")\n",
    "print(\"TEST benign:\", len(test_benign_df), \" (target:\", n_test_benign_target, \")\")\n",
    "print(\"Benign remaining for TRAIN:\", len(benign_remaining))\n",
    "\n",
    "\n",
    "# 6. BUILD TRAIN SET (1:1 BALANCED)\n",
    "# Training benign will be drawn from the remaining benign pool.\n",
    "max_train_pairs = min(len(phish_train_pool), len(benign_remaining))\n",
    "# You get this many 1:1 pairs\n",
    "train_phish_df  = phish_train_pool.iloc[:max_train_pairs].copy()\n",
    "train_benign_df = benign_remaining.iloc[:max_train_pairs].copy()\n",
    "\n",
    "print(\"\\nTRAIN phishing:\", len(train_phish_df))\n",
    "print(\"TRAIN benign  :\", len(train_benign_df))\n",
    "\n",
    "\n",
    "# 7. CONCAT & SHUFFLE FINAL SPLITS\n",
    "train_df = pd.concat([train_phish_df, train_benign_df], ignore_index=True)\n",
    "val_df   = pd.concat([val_recent_df, val_benign_df], ignore_index=True)\n",
    "test_df  = pd.concat([test_recent_df, test_benign_df], ignore_index=True)\n",
    "\n",
    "train_df = train_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "val_df   = val_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "test_df  = test_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFINAL SPLIT SIZES:\")\n",
    "print(\"  TRAIN:\", len(train_df), \" (phish:\", (train_df['label'] == 1).sum(),\n",
    "      \", benign:\", (train_df['label'] == 0).sum(), \")\")\n",
    "print(\"  VAL  :\", len(val_df),   \" (phish:\", (val_df['label'] == 1).sum(),\n",
    "      \", benign:\", (val_df['label'] == 0).sum(), \")\")\n",
    "print(\"  TEST :\", len(test_df),  \" (phish:\", (test_df['label'] == 1).sum(),\n",
    "      \", benign:\", (test_df['label'] == 0).sum(), \")\")\n",
    "\n",
    "\n",
    "# 8. SAVE\n",
    "train_df.to_csv(\"../processed_data/basic_data/train.csv\", index=False)\n",
    "val_df.to_csv(\"../processed_data/basic_data/val.csv\", index=False)\n",
    "test_df.to_csv(\"../processed_data/basic_data/testA.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved train.csv, val.csv, test.csv\")\n"
   ],
   "id": "e5a36dd1b55888a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent phishing: 46911\n",
      "Old phishing   : 114299\n",
      "Benign total   : 492895\n",
      "\n",
      "Recent phishing split:\n",
      "  Train recent phish: 32838\n",
      "  Val phish         : 4691\n",
      "  Test phish        : 9382\n",
      "\n",
      "Total phishing available for TRAIN: 147137\n",
      "\n",
      "VAL benign: 23455  (target: 23455 )\n",
      "TEST benign: 93820  (target: 93820 )\n",
      "Benign remaining for TRAIN: 375620\n",
      "\n",
      "TRAIN phishing: 147137\n",
      "TRAIN benign  : 147137\n",
      "\n",
      "FINAL SPLIT SIZES:\n",
      "  TRAIN: 294274  (phish: 147137 , benign: 147137 )\n",
      "  VAL  : 28146  (phish: 4691 , benign: 23455 )\n",
      "  TEST : 103202  (phish: 9382 , benign: 93820 )\n",
      "\n",
      "Saved train.csv, val.csv, test.csv\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:21:26.273004Z",
     "start_time": "2025-12-14T09:20:59.905805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from math import log2\n",
    "import os\n",
    "\n",
    "\n",
    "# Directories\n",
    "RAW_DIR = \"../processed_data/basic_data/\"\n",
    "OUT_DIR = \"../processed_data/engineered_data/\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Suspicious keywords\n",
    "SUSPICIOUS_KEYWORDS = [\n",
    "    \"login\", \"verify\", \"account\", \"update\", \"secure\",\n",
    "    \"bank\", \"signin\", \"password\", \"confirm\", \"safe\"\n",
    "]\n",
    "\n",
    "malicious_tlds = {\".ru\", \".tk\", \".ml\", \".xyz\", \".info\", \".top\", \".ga\", \".gq\", \".cf\"}\n",
    "\n",
    "# Feature Functions\n",
    "def is_ip(domain):\n",
    "    return bool(re.match(r\"^\\d{1,3}(\\.\\d{1,3}){3}$\", domain))\n",
    "\n",
    "def entropy(s):\n",
    "    if not s:\n",
    "        return 0\n",
    "    p = [s.count(c) / len(s) for c in set(s)]\n",
    "    return -sum(px * log2(px) for px in p)\n",
    "\n",
    "\n",
    "def count_subdomains(domain):\n",
    "    if not domain:\n",
    "        return 0\n",
    "    parts = domain.split(\".\")\n",
    "    return max(0, len(parts) - 2)\n",
    "\n",
    "\n",
    "def domain_features(url):\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc\n",
    "\n",
    "    if not domain:\n",
    "        return pd.Series([0, 0])  # domain_length_t2, malicious_tld_flag\n",
    "\n",
    "    domain_len = len(domain)\n",
    "    tld = \".\" + domain.split(\".\")[-1]\n",
    "    tld_flag = 1 if tld in malicious_tlds else 0\n",
    "\n",
    "    return pd.Series([domain_len, tld_flag])\n",
    "\n",
    "\n",
    "def encoded_char_flag(url):\n",
    "    encodings = [\"%20\", \"%2F\", \"%3D\", \"%3F\", \"%40\", \"%25\"]\n",
    "    return 1 if any(e in url for e in encodings) else 0\n",
    "\n",
    "\n",
    "def path_length_tier2(url):\n",
    "    parsed = urlparse(url)\n",
    "    return len(parsed.path) if parsed.path else 0\n",
    "\n",
    "\n",
    "def extract_tier1_features(url):\n",
    "    \"\"\"Extract Level-1 features (your original TIER-1).\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "    except:\n",
    "        return pd.Series([None] * 13)\n",
    "\n",
    "    domain = parsed.netloc\n",
    "    path = parsed.path\n",
    "\n",
    "    url_length        = len(url)\n",
    "    num_digits        = sum(c.isdigit() for c in url)\n",
    "    num_special_chars = sum(not c.isalnum() for c in url)\n",
    "    dot_count         = url.count(\".\")\n",
    "    hyphen_in_domain  = 1 if \"-\" in domain else 0\n",
    "    at_symbol         = 1 if \"@\" in url else 0\n",
    "    double_slash      = 1 if url.count(\"//\") > 1 else 0\n",
    "\n",
    "    subdomain_count   = domain.count(\".\") - 1 if domain else 0\n",
    "    domain_length     = len(domain)\n",
    "    path_length       = len(path)\n",
    "    ip_flag           = 1 if is_ip(domain) else 0\n",
    "    keyword_flag      = 1 if any(k in url.lower() for k in SUSPICIOUS_KEYWORDS) else 0\n",
    "    url_entropy       = entropy(url)\n",
    "\n",
    "    return pd.Series([\n",
    "        url_length, num_digits, num_special_chars, dot_count,\n",
    "        hyphen_in_domain, at_symbol, double_slash,\n",
    "        subdomain_count, domain_length, path_length,\n",
    "        ip_flag, keyword_flag, url_entropy\n",
    "    ])\n",
    "\n",
    "TIER1_COLS = [\n",
    "    \"url_length\",\n",
    "    \"num_digits\",\n",
    "    \"num_special_chars\",\n",
    "    \"dot_count\",\n",
    "    \"hyphen_in_domain\",\n",
    "    \"at_symbol\",\n",
    "    \"double_slash\",\n",
    "    \"subdomain_count_t1\",\n",
    "    \"domain_length_t1\",\n",
    "    \"path_length_t1\",\n",
    "    \"ip_flag\",\n",
    "    \"keyword_flag\",\n",
    "    \"url_entropy_t1\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# APPLY FEATURES\n",
    "def apply_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- Tier 1 ---\n",
    "    tier1 = df[\"url\"].apply(extract_tier1_features)\n",
    "    tier1.columns = TIER1_COLS\n",
    "    df = pd.concat([df, tier1], axis=1)\n",
    "\n",
    "    # --- Tier 2 ---\n",
    "    df[\"url_entropy\"] = df[\"url\"].apply(entropy)\n",
    "    df[\"subdomain_count\"] = df[\"url\"].apply(lambda x: count_subdomains(urlparse(x).netloc))\n",
    "\n",
    "    df[[\"domain_length_t2\", \"malicious_tld_flag\"]] = df[\"url\"].apply(domain_features)\n",
    "\n",
    "    df[\"encoded_flag\"] = df[\"url\"].apply(encoded_char_flag)\n",
    "    df[\"path_length_t2\"] = df[\"url\"].apply(path_length_tier2)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# LOAD SPLITS\n",
    "train = pd.read_csv(RAW_DIR + \"train.csv\")\n",
    "val   = pd.read_csv(RAW_DIR + \"val.csv\")\n",
    "testA = pd.read_csv(RAW_DIR + \"testA.csv\")\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(len(train), \"train\")\n",
    "print(len(val), \"val\")\n",
    "print(len(testA), \"testA\")\n",
    "\n",
    "\n",
    "\n",
    "# APPLY TO ALL SPLITS\n",
    "train_eng = apply_features(train)\n",
    "val_eng   = apply_features(val)\n",
    "test_eng  = apply_features(testA)\n",
    "\n",
    "\n",
    "# SAVE\n",
    "train_eng.to_csv(OUT_DIR + \"train.csv\", index=False)\n",
    "val_eng.to_csv(OUT_DIR + \"val.csv\", index=False)\n",
    "test_eng.to_csv(OUT_DIR + \"testA.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved engineered data to:\", OUT_DIR)\n"
   ],
   "id": "b85dd8dcbe1c9943",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n",
      "294274 train\n",
      "28146 val\n",
      "103202 testA\n",
      "\n",
      "Saved engineered data to: ../processed_data/engineered_data/\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:38:24.915775Z",
     "start_time": "2025-12-14T09:38:24.362570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"PhiUSIIL_Phishing_URL_Dataset.csv\")\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df[\"label\"].value_counts()"
   ],
   "id": "8c622606fff60198",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    134850\n",
       "0    100945\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:39:34.290090Z",
     "start_time": "2025-12-14T09:39:34.147271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = df[[\"url\", \"label\"]]\n",
    "df.to_csv(\"../processed_data/basic_data/testB.csv\")"
   ],
   "id": "eb7a501eb561663a",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "da7f668ac373986e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
